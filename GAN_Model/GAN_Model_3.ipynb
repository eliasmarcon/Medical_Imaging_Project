{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource\n",
    "\n",
    "- https://towardsdatascience.com/synthetic-data-generation-using-conditional-gan-45f91542ec6b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.layers import Activation, BatchNormalization, Concatenate, Dense, Dropout, Multiply, Embedding, Flatten, Input, Reshape, LeakyReLU, Conv2D, Conv2DTranspose\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define important parameters\n",
    "img_shape = (700, 460, 1)\n",
    "z_dim = 100\n",
    "n_class = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator CNN model\n",
    "def generator_model(z_dim):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(256 * 7 * 7, input_dim=z_dim,))\n",
    "    model.add(Reshape((7, 7, 256)))\n",
    "\n",
    "    model.add(Conv2DTranspose(128, 3, 2, padding='same',))\n",
    "    model.add(LeakyReLU(alpha = 0.01))\n",
    "\n",
    "    model.add(Conv2DTranspose(64, 3, 1, padding='same',))\n",
    "    model.add(LeakyReLU(alpha = 0.01))\n",
    "\n",
    "    model.add(Conv2DTranspose(1, 3, 2, padding='same',))\n",
    "    model.add(LeakyReLU(alpha = 0.01))\n",
    "\n",
    "    return model\n",
    "\n",
    "# generator input \n",
    "def generator(z_dim):\n",
    "    # latent input\n",
    "    z = Input(shape=(z_dim, ))\n",
    "    # label input\n",
    "    label = Input(shape=(1, ), dtype='int32')\n",
    "    # convert label to embedding\n",
    "    label_embedding = Embedding(n_class, z_dim)(label)\n",
    "\n",
    "    label_embedding = Flatten()(label_embedding)\n",
    "    # dot product two inputs\n",
    "    joined_representation = Multiply()([z, label_embedding])\n",
    "\n",
    "    generator = generator_model(z_dim)\n",
    "\n",
    "    conditioned_img = generator(joined_representation)\n",
    "\n",
    "    model =  Model([z, label], conditioned_img)\n",
    "    \n",
    "    # save model blueprint to image\n",
    "    # plot_model(model,'generator.jpg',show_shapes=True,show_dtype=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator CNN model\n",
    "def discriminator_model(img_shape):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(64,3,2,input_shape=(img_shape[0], img_shape[1], img_shape[2] + 1),))\n",
    "    model.add(LeakyReLU(alpha = 0.01))\n",
    "\n",
    "    model.add(Conv2D(64,3,2,input_shape=img_shape,padding='same',))\n",
    "    model.add(LeakyReLU(alpha = 0.001))\n",
    "\n",
    "    model.add(Conv2D(128,3,2,input_shape=img_shape,padding='same',))\n",
    "    model.add(LeakyReLU(alpha = 0.001))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def discriminator(img_shape):\n",
    "    # image input\n",
    "    img = Input(shape=img_shape)\n",
    "    # label input\n",
    "    label = Input(shape=(1, ), dtype='int32')\n",
    "\n",
    "    label_embedding = Embedding(n_class, np.prod(img_shape),input_length=1)(label)\n",
    "\n",
    "    label_embedding = Flatten()(label_embedding)\n",
    "\n",
    "    label_embedding = Reshape(img_shape)(label_embedding)\n",
    "    # concatenate the image and label\n",
    "    concatenated = Concatenate(axis=-1)([img, label_embedding])\n",
    "\n",
    "    discriminator = discriminator_model(img_shape)\n",
    "\n",
    "    classification = discriminator(concatenated)\n",
    "\n",
    "    model = Model([img, label], classification)\n",
    "\n",
    "    # plot_model(model,'discriminator.jpg',show_shapes=True,show_dtype=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a complete GAN architecture\n",
    "def cgan(generator, discriminator):\n",
    "\n",
    "    z = Input(shape=(z_dim, ))\n",
    "\n",
    "    label = Input(shape=(1, ))\n",
    "\n",
    "    img = generator([z, label])\n",
    "\n",
    "    classification = discriminator([img, label])\n",
    "\n",
    "    model = Model([z, label], classification)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 700, 460, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 700, 460, 1), dtype=tf.float32, name='input_7'), name='input_7', description=\"created by layer 'input_7'\"), but it was called on an input with incompatible shape (None, 28, 28, 1).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'concatenate_1' (type Concatenate).\n\nDimension 1 in both shapes must be equal, but are 28 and 700. Shapes are [?,28,28] and [?,700,460]. for '{{node model_2/concatenate_1/concat}} = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32](Placeholder, model_2/reshape_2/Reshape, model_2/concatenate_1/concat/axis)' with input shapes: [?,28,28,1], [?,700,460,1], [] and with computed input tensors: input[2] = <3>.\n\nCall arguments received by layer 'concatenate_1' (type Concatenate):\n  • inputs=['tf.Tensor(shape=(None, 28, 28, 1), dtype=float32)', 'tf.Tensor(shape=(None, 700, 460, 1), dtype=float32)']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1632/2349081656.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# compile the whole C-GAN architectu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mcgan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcgan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mcgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1632/117048542.py\u001b[0m in \u001b[0;36mcgan\u001b[1;34m(generator, discriminator)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mclassification\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassification\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\elias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\elias\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\backend.py\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(tensors, axis)\u001b[0m\n\u001b[0;32m   3570\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3571\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3572\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3574\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer 'concatenate_1' (type Concatenate).\n\nDimension 1 in both shapes must be equal, but are 28 and 700. Shapes are [?,28,28] and [?,700,460]. for '{{node model_2/concatenate_1/concat}} = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32](Placeholder, model_2/reshape_2/Reshape, model_2/concatenate_1/concat/axis)' with input shapes: [?,28,28,1], [?,700,460,1], [] and with computed input tensors: input[2] = <3>.\n\nCall arguments received by layer 'concatenate_1' (type Concatenate):\n  • inputs=['tf.Tensor(shape=(None, 28, 28, 1), dtype=float32)', 'tf.Tensor(shape=(None, 700, 460, 1), dtype=float32)']"
     ]
    }
   ],
   "source": [
    "discriminator = discriminator(img_shape)\n",
    "# compile the discriminator architecture \n",
    "discriminator.compile(loss='binary_crossentropy',\n",
    "                      optimizer=Adam(),\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "generator = generator(z_dim)\n",
    "# set discriminator to non-trainanle \n",
    "discriminator.trainable = False\n",
    "# compile the whole C-GAN architectu\n",
    "cgan = cgan(generator, discriminator)\n",
    "cgan.compile(loss='binary_crossentropy', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "\n",
    "# def load_data(path1, path2):\n",
    "\n",
    "#     files = []\n",
    "\n",
    "#     paths = [path1, path2]\n",
    "\n",
    "#     for path in paths:\n",
    "\n",
    "#         for folder in os.listdir(path):\n",
    "\n",
    "#             if \".\" not in folder: \n",
    "\n",
    "#                 for image in os.listdir(path + folder):\n",
    "\n",
    "#                     img = cv2.imread(path + folder + \"/\" + image)\n",
    "\n",
    "#                     files.append(np.array(img))\n",
    "\n",
    "#     return files\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "def load_data():\n",
    "\n",
    "    images = []\n",
    "\n",
    "    path = 'D:/Medical_Imaging_Projekt/Dataset/benign/'\n",
    "\n",
    "    for folder in os.listdir(path):\n",
    "\n",
    "        if \".\" not in folder: \n",
    "\n",
    "            for image in os.listdir(path + folder):\n",
    "\n",
    "                img = cv2.imread(path + folder + \"/\" + image)\n",
    "\n",
    "                images.append(np.array(img))\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label to category dictionary\n",
    "dict_cancer = {0: \"benign\", 1: \"malignant\"}\n",
    "\n",
    "# function to plot and save sample images\n",
    "def plot_sample_images(epoch ,rows=5,columns=4):\n",
    "\n",
    "    z = np.random.normal(0, 1, (rows * columns, z_dim))\n",
    "    a =np.arange(0,10)\n",
    "    b =np.arange(0,10)\n",
    "\n",
    "    labels = np.append(a,b).reshape(-1,1)\n",
    "    \n",
    "    gen_imgs = generator.predict([z, labels])\n",
    "\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "    print(\"Epoch : %d \"%(epoch+1))\n",
    "    fig, axs = plt.subplots(rows,\n",
    "                            columns,\n",
    "                            figsize =(50, 20),\n",
    "                            sharey=True,\n",
    "                            sharex=True)\n",
    "\n",
    "    cnt = 0\n",
    "    for i in range(rows):\n",
    "        for j in range(columns):\n",
    "            axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
    "            axs[i, j].axis('off')\n",
    "            axs[i, j].set_title(\"Type: %s\" % dict_cancer.get(labels[cnt][0]))\n",
    "            cnt += 1\n",
    "    fig.savefig('image%d.jpg'%(epoch))\n",
    "\n",
    " \n",
    "# define training step\n",
    "def train(epochs, batch_size, sample_interval):\n",
    "\n",
    "    #  import Fashion-MNIST dataset\n",
    "    data = load_data()\n",
    "\n",
    "    split = 0.8\n",
    "    X_train = data[ : int(len(data) * split)]\n",
    "    Y_train = data[int(len(data) * split) : ]\n",
    "\n",
    "    X_train = X_train.astype(\"float32\") / 255.0\n",
    "    X_train = np.reshape(X_train, (-1, 28, 28, 1))\n",
    "\n",
    "\n",
    "    real = np.ones((batch_size, 1))\n",
    "\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "    \n",
    "    for epoch in range(epochs): \n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        imgs, labels = X_train[idx], Y_train[idx]\n",
    "\n",
    "        z = np.random.normal(0, 1, (batch_size, z_dim))\n",
    "        # generate images from generator\n",
    "        gen_imgs = generator.predict([z, labels])\n",
    "        # pass real an generated images to the discriminator and ctrain on them\n",
    "        d_loss_real = discriminator.train_on_batch([imgs, labels], real)\n",
    "        d_loss_fake = discriminator.train_on_batch([gen_imgs, labels], fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        z = np.random.normal(0, 1, (batch_size, z_dim))\n",
    "\n",
    "        labels = np.random.randint(0, n_class, batch_size).reshape(-1, 1)\n",
    "   \n",
    "        g_loss = cgan.train_on_batch([z, labels], real)\n",
    "\n",
    "        if (epoch + 1) % sample_interval == 0:\n",
    "\n",
    "            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" %(epoch + 1, d_loss[0], 100 * d_loss[1], g_loss))\n",
    "\n",
    "            plot_sample_images(epoch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10 #20000\n",
    "batch_size = 128\n",
    "sample_interval = 2 # 2000\n",
    "\n",
    "train(epochs, batch_size, sample_interval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3fc63de107a3995f66d33859f380f73bd26ac40da4b187ce627a4472b1acbfe0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
